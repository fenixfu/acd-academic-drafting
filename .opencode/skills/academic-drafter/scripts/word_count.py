# utf-8
#!/usr/bin/env python3
"""
å­—æ•°ç»Ÿè®¡ç¨‹åº - ç”¨äºæ ¸éªŒä¸­æ–‡å­¦æœ¯æ–‡æœ¬å­—æ•°

è®¡æ•°è§„åˆ™ï¼š
  - ä¸­æ–‡å­—ç¬¦ï¼ˆå«æ ‡ç‚¹ï¼‰: æ¯ä¸ªè®¡ 1 å­—
  - è‹±æ–‡å•è¯ï¼ˆè¿ç»­å­—æ¯åºåˆ—ï¼‰: æ¯ä¸ªè®¡ 2 å­—ï¼ˆç­‰æ•ˆæ¢ç®—ï¼‰
  - æ•°å­—å­—ç¬¦: æ¯ä½è®¡ 1 å­—
  - æ ‡ç‚¹ç¬¦å·ç­‰å…¶ä»–å­—ç¬¦: ä¸è®¡å…¥
  - ç©ºæ ¼ä¸è®¡å…¥

åˆ†åŒºé€»è¾‘ï¼š
  - æ­£æ–‡ï¼šä»ç¬¬ä¸€ä¸ªéå…ƒæ•°æ®è¡Œèµ·ï¼Œåˆ°å‚è€ƒæ–‡çŒ®èŠ‚å‰
  - å‚è€ƒæ–‡çŒ®ï¼šä»¥ "å‚è€ƒæ–‡çŒ®"ã€"References"ã€"Bibliography" æ ‡é¢˜è¡Œæˆ–
    è¿ç»­ç¼–å·åˆ—è¡¨ï¼ˆ"[1]" / "1."ï¼‰å¼€å¤´çš„åŒºæ®µ
  - åˆ†åˆ«ç»Ÿè®¡åç»™å‡ºåˆè®¡æ€»å’Œ

ç”¨æ³•ï¼š
  python word_count.py [æ–‡ä»¶è·¯å¾„ ...]          # ç»Ÿè®¡ä¸€ä¸ªæˆ–å¤šä¸ªæ–‡ä»¶
  python word_count.py --text "æ–‡æœ¬å†…å®¹"        # ç›´æ¥ç»Ÿè®¡æ–‡æœ¬ä¸²ï¼ˆä¸åˆ†åŒºï¼Œæ•´ä½“è®¡æ•°ï¼‰
  python word_count.py --limit 1500 è®ºæ–‡.md     # æŒ‡å®šå­—æ•°ä¸Šé™ï¼ˆé»˜è®¤ 800ï¼‰
"""

import argparse
import re
import sys
from pathlib import Path

# â”€â”€ è®¡æ•°é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENGLISH_WORD_EQUIV = 2   # 1 è‹±æ–‡å•è¯ç­‰æ•ˆ N å­—
DEFAULT_WORD_LIMIT = 800 # é»˜è®¤å­—æ•°ä¸Šé™


def strip_markdown(text: str) -> str:
    """ç§»é™¤ markdown æ ¼å¼æ ‡è®°ï¼Œä¿ç•™çº¯æ–‡æœ¬å†…å®¹ã€‚"""
    # æ ‡é¢˜æ ‡è®° (# ## ### ç­‰è¡Œé¦–äº•å·)
    text = re.sub(r"^#{1,6}\s*", "", text, flags=re.MULTILINE)
    # ç²—ä½“/æ–œä½“æ ‡è®° (** * __ _)
    text = re.sub(r"\*{1,3}|_{1,3}", "", text)
    # åˆ é™¤çº¿ ~~
    text = re.sub(r"~~", "", text)
    # è¡Œå†…ä»£ç  `
    text = re.sub(r"`", "", text)
    # å¼•ç”¨æ ‡è®° >
    text = re.sub(r"^>\s*", "", text, flags=re.MULTILINE)
    # æ— åºåˆ—è¡¨æ ‡è®° (- * + è¡Œé¦–)
    text = re.sub(r"^[\-\*\+]\s+", "", text, flags=re.MULTILINE)
    # é“¾æ¥ [text](url) â†’ text
    text = re.sub(r"\[([^\]]*)\]\([^)]*\)", r"\1", text)
    # å›¾ç‰‡ ![alt](url) â†’ ç§»é™¤
    text = re.sub(r"!\[([^\]]*)\]\([^)]*\)", "", text)
    # æ°´å¹³çº¿ --- ***
    text = re.sub(r"^[-*_]{3,}\s*$", "", text, flags=re.MULTILINE)
    return text


def count_text(text: str) -> dict:
    """
    ç»Ÿè®¡ä¸€æ®µæ–‡æœ¬çš„ç­‰æ•ˆå­—æ•°ã€‚
    è‡ªåŠ¨å‰¥ç¦» markdown æ ¼å¼æ ‡è®°åå†è®¡æ•°ã€‚

    Returns dict with keys:
        chinese_chars, english_words, english_equiv,
        digit_chars, total
    """
    text = strip_markdown(text)

    chinese_chars = len(re.findall(
        r"[\u4e00-\u9fff]", text
    ))
    # è‹±æ–‡å•è¯ï¼šä»…åŒ¹é…è¿ç»­å­—æ¯åºåˆ—ï¼ˆä¸å«æ•°å­—ï¼‰
    english_words = len(re.findall(r"[a-zA-Z]+", text))
    # æ•°å­—å­—ç¬¦ï¼šæ¯ä½å•ç‹¬è®¡ 1 å­—
    digit_chars = len(re.findall(r"[0-9]", text))
    # æ ‡ç‚¹ç¬¦å·ç­‰å…¶ä»–å­—ç¬¦ä¸è®¡å…¥

    english_equiv = english_words * ENGLISH_WORD_EQUIV
    total = chinese_chars + english_equiv + digit_chars

    return {
        "chinese_chars": chinese_chars,
        "english_words": english_words,
        "english_equiv": english_equiv,
        "digit_chars": digit_chars,
        "total": total,
    }


# â”€â”€ Markdown åˆ†åŒºæå– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_REF_HEADING_RE = re.compile(
    r"^#{1,6}\s*(å‚è€ƒæ–‡çŒ®|References|Bibliography)\s*$",
    re.IGNORECASE,
)
_REF_INLINE_RE = re.compile(
    r"^[\*\[\ã€\(\ï¼ˆ\{\<]*?(å‚è€ƒæ–‡çŒ®|References|Bibliography)\S*?\s*$",
    re.IGNORECASE,
)
# è¿ç»­ç¼–å·åˆ—è¡¨é¦–è¡Œï¼š[1] æˆ– 1.
_REF_NUMBERED_RE = re.compile(r"^\[1\]|^1\.\s")

# å…ƒæ•°æ®è¡Œï¼ˆæ ‡é¢˜è¡Œã€blockquoteã€yaml fence ç­‰ï¼Œæ’é™¤æ­£æ–‡èµ·å§‹ç‚¹ä¹‹åçš„æ ‡é¢˜ï¼‰
_META_RE = re.compile(r"^(#|>|---|```|<!--)")


def split_sections(content: str):
    """
    å°† markdown å†…å®¹æ‹†åˆ†ä¸º (æ­£æ–‡æ–‡æœ¬, å‚è€ƒæ–‡çŒ®æ–‡æœ¬)ã€‚

    é€»è¾‘ï¼š
    1. è·³è¿‡æ–‡ä»¶å¼€å¤´çš„å…ƒæ•°æ®è¡Œï¼ˆ#ã€>ã€--- ç­‰ï¼‰
    2. é‡åˆ°å‚è€ƒæ–‡çŒ®èŠ‚æ ‡è¯†æ—¶åˆ‡æ¢åˆ°å‚è€ƒæ–‡çŒ®åŒº
    3. è¿”å›ä¸¤æ®µçº¯æ–‡æœ¬
    """
    lines = content.splitlines()

    body_lines = []
    ref_lines = []
    in_ref = False
    body_started = False

    for line in lines:
        stripped = line.strip()

        # æ£€æµ‹å‚è€ƒæ–‡çŒ®èŠ‚å¼€å§‹
        if not in_ref and (
            _REF_HEADING_RE.match(stripped)
            or _REF_INLINE_RE.match(stripped)
        ):
            in_ref = True
            ref_lines.append(line)
            continue

        if in_ref:
            ref_lines.append(line)
            continue

        # æ­£æ–‡å°šæœªå¼€å§‹æ—¶è·³è¿‡å…ƒæ•°æ®è¡Œ
        if not body_started:
            if stripped and not _META_RE.match(stripped):
                body_started = True
            else:
                continue  # è·³è¿‡å…ƒæ•°æ®

        # æ­£æ–‡ä¸­å‡ºç°ç¼–å·åˆ—è¡¨é¦–è¡Œï¼Œä¸”å·²æœ‰è¶³å¤Ÿæ­£æ–‡ï¼Œè§†ä¸ºå‚è€ƒæ–‡çŒ®å¼€å§‹
        if body_started and _REF_NUMBERED_RE.match(stripped):
            # åªæœ‰åœ¨æ­£æ–‡å·²æœ‰å†…å®¹æ—¶æ‰åˆ‡æ¢ï¼ˆé¿å…è¯¯åˆ¤æ­£æ–‡å†…ç¼–å·ï¼‰
            if len(body_lines) > 5:
                in_ref = True
                ref_lines.append(line)
                continue

        body_lines.append(line)

    return "\n".join(body_lines), "\n".join(ref_lines)


# â”€â”€ æ˜¾ç¤ºè¾…åŠ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fmt_stats(stats: dict, label: str, limit: int | None = None, tolerance: float = 5.0, boundary: str = "hard") -> None:
    print(f"\n  ã€{label}ã€‘")
    print(f"    ä¸­æ–‡å­—ç¬¦  : {stats['chinese_chars']} å­—")
    print(f"    è‹±æ–‡å•è¯  : {stats['english_words']} è¯ Ã— {ENGLISH_WORD_EQUIV} = {stats['english_equiv']} å­—ï¼ˆç­‰æ•ˆï¼‰")
    print(f"    æ•°å­—å­—ç¬¦  : {stats['digit_chars']} ä½ Ã— 1 = {stats['digit_chars']} å­—")
    print( "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"    å°è®¡      : {stats['total']} å­—")
    if limit is not None:
        if boundary == "hard":
            min_limit = limit * (100 - 2 * tolerance) / 100.0
            max_limit = float(limit)
        else: # soft boundary
            min_limit = limit * (100 - tolerance) / 100.0
            max_limit = limit * (100 + tolerance) / 100.0
            
        if stats["total"] < min_limit:
            missing = min_limit - stats["total"]
            print(f"    âŒ å­—æ•°ä¸è¶³ï¼ˆä¸¥é‡æµªè´¹ï¼‰ï¼Œè¦æ±‚ä¸‹é™ {min_limit:.1f} å­—ï¼Œå½“å‰ç¼ºå°‘ {missing:.1f} å­—")
        elif stats["total"] > max_limit:
            excess = stats["total"] - max_limit
            print(f"    âŒ è¶…å‡ºå­—æ•°ä¸Šé™ {max_limit:.1f} å­—ï¼Œè¶…å‡º {excess:.1f} å­—")
        else:
            print(f"    âœ… ç¬¦åˆå­—æ•°é™åˆ¶ï¼ˆèŒƒå›´ [{min_limit:.1f}, {max_limit:.1f}] å­—ï¼‰")
            


def check_file(filepath: str, limit: int = DEFAULT_WORD_LIMIT, tolerance: float = 5.0, boundary: str = "hard") -> int:
    """æ£€æŸ¥æ–‡ä»¶ï¼Œåˆ†åˆ«ç»Ÿè®¡æ­£æ–‡ä¸å‚è€ƒæ–‡çŒ®ï¼Œè¿”å›åˆè®¡å­—æ•°ã€‚"""
    path = Path(filepath)
    if not path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return 0

    content = path.read_text(encoding="utf-8")
    body_text, ref_text = split_sections(content)

    body_stats = count_text(body_text)
    ref_stats = count_text(ref_text)
    combined_total = body_stats["total"] + ref_stats["total"]

    print(f"\n{'=' * 60}")
    print(f"ğŸ“„ æ–‡ä»¶: {filepath}")
    print(f"{'=' * 60}")

    fmt_stats(body_stats, "æ­£æ–‡")
    fmt_stats(ref_stats,  "å‚è€ƒæ–‡çŒ®")
    print(f"\n  ã€æ­£æ–‡+å‚è€ƒæ–‡çŒ®ã€‘")
    print(f"    æ€»è®¡      : {combined_total} å­—")
    
    if boundary == "hard":
        min_limit = limit * (100 - 2 * tolerance) / 100.0
        max_limit = float(limit)
    else: # soft boundary
        min_limit = limit * (100 - tolerance) / 100.0
        max_limit = limit * (100 + tolerance) / 100.0

    if combined_total < min_limit:
        missing = min_limit - combined_total
        print(f"    âŒ å­—æ•°ä¸è¶³ï¼ˆä¸¥é‡æµªè´¹ï¼‰ï¼Œè¦æ±‚ä¸‹é™ {min_limit:.1f} å­—ï¼Œå½“å‰æ€»è®¡ç¼ºå°‘ {missing:.1f} å­—")
    elif combined_total > max_limit:
        excess = combined_total - max_limit
        print(f"    âŒ è¶…å‡ºå­—æ•°ä¸Šé™ {max_limit:.1f} å­—ï¼Œå½“å‰æ€»è®¡è¶…å‡º {excess:.1f} å­—")
    else:
        print(f"    âœ… ç¬¦åˆå­—æ•°é™åˆ¶ï¼ˆèŒƒå›´ [{min_limit:.1f}, {max_limit:.1f}] å­—ï¼‰")

    print(f"{'=' * 60}")

    return combined_total


def check_text(text: str, limit: int = DEFAULT_WORD_LIMIT, tolerance: float = 5.0, boundary: str = "hard") -> int:
    """ç›´æ¥ç»Ÿè®¡æ–‡æœ¬ä¸²ï¼ˆä¸åˆ†åŒºï¼‰ï¼Œè¿”å›å­—æ•°ã€‚"""
    stats = count_text(text)

    print(f"\n{'=' * 60}")
    print(f"ğŸ“ æ–‡æœ¬ä¸²ç»Ÿè®¡ï¼ˆä¸åˆ†æ­£æ–‡/å‚è€ƒæ–‡çŒ®åŒºï¼‰")
    print(f"{'=' * 60}")
    fmt_stats(stats, "å…¨æ–‡", limit, tolerance, boundary)
    print(f"{'=' * 60}")

    return stats["total"]


# â”€â”€ å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(
        description="å­¦æœ¯æ–‡æœ¬å­—æ•°ç»Ÿè®¡å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  python word_count.py è®ºæ–‡.md                      # ç»Ÿè®¡æ–‡ä»¶ï¼Œé»˜è®¤ä¸Šé™ 800
  python word_count.py --limit 1500 è®ºæ–‡.md          # æŒ‡å®šä¸Šé™ 1500
  python word_count.py æ‘˜è¦.md æ­£æ–‡.md               # åŒæ—¶ç»Ÿè®¡å¤šä¸ªæ–‡ä»¶
  python word_count.py --text "è¿™æ˜¯ä¸€æ®µéœ€è¦ç»Ÿè®¡çš„æ–‡æœ¬"  # ç›´æ¥ç»Ÿè®¡æ–‡æœ¬ä¸²
  python word_count.py --limit 500 --text "$(cat æ‘˜è¦.md)"  # ç®¡é“ä¼ å…¥æ–‡æœ¬
        """,
    )
    parser.add_argument(
        "files",
        nargs="*",
        metavar="æ–‡ä»¶è·¯å¾„",
        help="è¦ç»Ÿè®¡çš„æ–‡ä»¶ï¼ˆå¯å¤šä¸ªï¼‰",
    )
    parser.add_argument(
        "--text", "-t",
        metavar="æ–‡æœ¬å†…å®¹",
        help="ç›´æ¥ä¼ å…¥æ–‡æœ¬ä¸²è¿›è¡Œç»Ÿè®¡ï¼ˆä¸åˆ†æ­£æ–‡/å‚è€ƒæ–‡çŒ®åŒºï¼‰",
    )
    parser.add_argument(
        "--limit", "-l",
        type=int,
        default=DEFAULT_WORD_LIMIT,
        metavar="å­—æ•°ä¸Šé™",
        help=f"å­—æ•°ä¸Šé™ï¼ˆé»˜è®¤ {DEFAULT_WORD_LIMIT}ï¼‰",
    )
    parser.add_argument(
        "--tolerance", "-x",
        type=float,
        default=5.0,
        metavar="å®½å®¹åŒºé—´",
        help="å®½å®¹åŒºé—´ç™¾åˆ†æ¯” (é»˜è®¤ 5.0)",
    )
    parser.add_argument(
        "--boundary", "-b",
        choices=["hard", "soft"],
        default="hard",
        metavar="è¾¹ç•Œç±»å‹",
        help="è¾¹ç•Œè®¡ç®—æ–¹å¼ (hard/softï¼Œé»˜è®¤ hard)",
    )

    args = parser.parse_args()

    print("ğŸ“ å­¦æœ¯æ–‡æœ¬å­—æ•°ç»Ÿè®¡å·¥å…·")
    print("=" * 60)
    print(f"è®¡æ•°è§„åˆ™ï¼šè‹±æ–‡å•è¯ Ã— {ENGLISH_WORD_EQUIV}ï¼Œä¸­æ–‡å­—ç¬¦ Ã— 1ï¼Œæ•°å­—å­—ç¬¦ Ã— 1ï¼Œæ ‡ç‚¹ç­‰ç¬¦å·ä¸è®¡")
    
    if args.boundary == 'hard':
        print(f"å­—æ•°é™åˆ¶ï¼š{args.limit} å­— (è¾¹ç•Œç±»å‹: ç¡¬è¾¹ç•Œï¼Œå®½å®¹åº¦ {args.tolerance}%)")
    else:
        print(f"å­—æ•°é™åˆ¶ï¼š{args.limit} å­— (è¾¹ç•Œç±»å‹: è½¯è¾¹ç•Œï¼Œå®½å®¹åº¦ {args.tolerance}%)")
    print("=" * 60)

    # --text æ¨¡å¼ï¼šç›´æ¥ç»Ÿè®¡æ–‡æœ¬ä¸²
    if args.text is not None:
        if args.files:
            print("âš ï¸  åŒæ—¶æŒ‡å®šäº† --text å’Œæ–‡ä»¶è·¯å¾„ï¼Œä»…ç»Ÿè®¡ --text å†…å®¹ï¼Œå¿½ç•¥æ–‡ä»¶è·¯å¾„ã€‚")
        check_text(args.text, limit=args.limit, tolerance=args.tolerance, boundary=args.boundary)
        return

    # æ–‡ä»¶æ¨¡å¼
    if args.files:
        for filepath in args.files:
            check_file(filepath, limit=args.limit, tolerance=args.tolerance, boundary=args.boundary)
        return

    # æ— å‚æ•°ï¼šå°è¯•è‡ªåŠ¨å®šä½æœ€æ–°æ–‡ä»¶
    try:
        project_root = Path(__file__).resolve().parents[4]
        candidates = list(project_root.glob("output/*.md"))

        if not candidates:
            print("\nâš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶ï¼Œè¯·æŒ‡å®šè·¯å¾„:")
            print("  python word_count.py <æ–‡ä»¶è·¯å¾„>")
            print("  python word_count.py --text 'æ–‡æœ¬å†…å®¹'")
            return

        target = max(candidates, key=lambda p: p.stat().st_mtime)
        print(f"\nè‡ªåŠ¨é€‰å–æœ€æ–°æ–‡ä»¶: {target}")
        check_file(str(target), limit=args.limit, tolerance=args.tolerance, boundary=args.boundary)
    except IndexError:
        print("\nâš ï¸ æ— æ³•è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½• (parents[4] è¶Šç•Œ)ã€‚")
        print("è¯·ç›´æ¥æä¾›æ–‡ä»¶è·¯å¾„: python word_count.py <æ–‡ä»¶è·¯å¾„>")


if __name__ == "__main__":
    main()
